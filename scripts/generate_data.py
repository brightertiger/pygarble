#!/usr/bin/env python3
"""
Data generation script for pygarble.

Downloads word frequency data from Peter Norvig's collection (MIT licensed)
and generates embedded lookup tables for the library.

Data source: https://norvig.com/ngrams/
License: MIT (free to use)

This script should be run at development time, not at runtime.
The generated files are committed to the repository.
"""

import json
import math
import os
import re
import urllib.request
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, Set, Tuple

# URLs for data sources
NORVIG_WORD_FREQ_URL = "https://norvig.com/ngrams/count_1w.txt"

# Output paths
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
DATA_DIR = PROJECT_ROOT / "pygarble" / "data"


def download_word_frequencies(url: str) -> Dict[str, int]:
    """Download word frequency data from Norvig's site."""
    print(f"Downloading word frequencies from {url}...")

    with urllib.request.urlopen(url) as response:
        content = response.read().decode("utf-8")

    word_freq = {}
    for line in content.strip().split("\n"):
        parts = line.strip().split("\t")
        if len(parts) == 2:
            word, count = parts
            word = word.lower().strip()
            # Only include alphabetic words
            if word.isalpha() and len(word) >= 2:
                word_freq[word] = int(count)

    print(f"  Downloaded {len(word_freq)} words")
    return word_freq


def generate_english_words(word_freq: Dict[str, int], max_words: int = 50000) -> Set[str]:
    """
    Generate set of common English words.

    Uses top N words by frequency, filtered for quality.
    """
    print(f"Generating English word set (top {max_words})...")

    # Sort by frequency and take top N
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

    words = set()
    for word, freq in sorted_words[:max_words]:
        # Additional quality filters
        if len(word) >= 2 and word.isalpha():
            words.add(word)

    print(f"  Generated {len(words)} words")
    return words


def compute_bigram_probabilities(word_freq: Dict[str, int]) -> Dict[str, float]:
    """
    Compute log-probability transition matrix for character bigrams.

    Uses Laplace smoothing to handle unseen bigrams.
    Returns log probabilities for numerical stability.
    """
    print("Computing bigram transition probabilities...")

    # Count all bigrams weighted by word frequency
    bigram_counts: Dict[Tuple[str, str], int] = defaultdict(int)
    char_counts: Dict[str, int] = defaultdict(int)

    # Process words weighted by their frequency
    for word, freq in word_freq.items():
        word = word.lower()
        # Add word boundaries
        padded = " " + word + " "
        for i in range(len(padded) - 1):
            c1, c2 = padded[i], padded[i + 1]
            if c1.isalpha() or c1 == " ":
                if c2.isalpha() or c2 == " ":
                    bigram_counts[(c1, c2)] += freq
                    char_counts[c1] += freq

    # All possible characters (26 letters + space)
    chars = " abcdefghijklmnopqrstuvwxyz"
    vocab_size = len(chars)

    # Compute log probabilities with Laplace smoothing
    bigram_log_probs = {}
    smoothing = 1  # Laplace smoothing parameter

    for c1 in chars:
        total = char_counts.get(c1, 0) + smoothing * vocab_size
        for c2 in chars:
            count = bigram_counts.get((c1, c2), 0) + smoothing
            prob = count / total
            log_prob = math.log(prob)
            bigram_log_probs[c1 + c2] = round(log_prob, 6)

    print(f"  Generated {len(bigram_log_probs)} bigram probabilities")
    return bigram_log_probs


def compute_trigram_frequencies(word_freq: Dict[str, int], top_n: int = 2000) -> Set[str]:
    """
    Compute set of most common character trigrams.
    """
    print(f"Computing top {top_n} trigram frequencies...")

    trigram_counts: Counter = Counter()

    for word, freq in word_freq.items():
        word = word.lower()
        if len(word) >= 3:
            for i in range(len(word) - 2):
                trigram = word[i:i+3]
                if trigram.isalpha():
                    trigram_counts[trigram] += freq

    # Get top N trigrams
    top_trigrams = set(t for t, _ in trigram_counts.most_common(top_n))

    print(f"  Generated {len(top_trigrams)} common trigrams")
    return top_trigrams


def write_words_file(words: Set[str], filepath: Path) -> None:
    """Write English words as a Python module."""
    print(f"Writing words to {filepath}...")

    # Sort for deterministic output
    sorted_words = sorted(words)

    content = '''"""
English word set for garble detection.

This file is auto-generated by scripts/generate_data.py
Data source: Peter Norvig's word frequency list (https://norvig.com/ngrams/)
License: MIT

Do not edit this file manually.
"""

# fmt: off
ENGLISH_WORDS = frozenset({
'''

    # Write words in chunks for readability
    chunk_size = 10
    for i in range(0, len(sorted_words), chunk_size):
        chunk = sorted_words[i:i+chunk_size]
        line = "    " + ", ".join(f'"{w}"' for w in chunk) + ","
        content += line + "\n"

    content += """})
# fmt: on
"""

    filepath.write_text(content)
    print(f"  Written {len(words)} words ({filepath.stat().st_size / 1024:.1f} KB)")


def write_bigrams_file(bigram_probs: Dict[str, float], filepath: Path) -> None:
    """Write bigram probabilities as a Python module."""
    print(f"Writing bigrams to {filepath}...")

    content = '''"""
Character bigram log-probabilities for Markov chain garble detection.

This file is auto-generated by scripts/generate_data.py
Data source: Peter Norvig's word frequency list (https://norvig.com/ngrams/)
License: MIT

Log probabilities are used for numerical stability.
To compute text probability: sum(BIGRAM_LOG_PROBS.get(bigram, DEFAULT_LOG_PROB) for bigram in text_bigrams)

Do not edit this file manually.
"""

# Default log probability for unseen bigrams (very unlikely)
DEFAULT_LOG_PROB = -10.0

# fmt: off
BIGRAM_LOG_PROBS = {
'''

    # Sort for deterministic output
    sorted_bigrams = sorted(bigram_probs.items())

    # Write in chunks
    chunk_size = 8
    for i in range(0, len(sorted_bigrams), chunk_size):
        chunk = sorted_bigrams[i:i+chunk_size]
        pairs = [f'"{k}": {v}' for k, v in chunk]
        content += "    " + ", ".join(pairs) + ",\n"

    content += """}
# fmt: on
"""

    filepath.write_text(content)
    print(f"  Written {len(bigram_probs)} bigrams ({filepath.stat().st_size / 1024:.1f} KB)")


def write_trigrams_file(trigrams: Set[str], filepath: Path) -> None:
    """Write common trigrams as a Python module."""
    print(f"Writing trigrams to {filepath}...")

    sorted_trigrams = sorted(trigrams)

    content = '''"""
Common English character trigrams for garble detection.

This file is auto-generated by scripts/generate_data.py
Data source: Peter Norvig's word frequency list (https://norvig.com/ngrams/)
License: MIT

Do not edit this file manually.
"""

# fmt: off
COMMON_TRIGRAMS = frozenset({
'''

    # Write in chunks
    chunk_size = 15
    for i in range(0, len(sorted_trigrams), chunk_size):
        chunk = sorted_trigrams[i:i+chunk_size]
        line = "    " + ", ".join(f'"{t}"' for t in chunk) + ","
        content += line + "\n"

    content += """})
# fmt: on
"""

    filepath.write_text(content)
    print(f"  Written {len(trigrams)} trigrams ({filepath.stat().st_size / 1024:.1f} KB)")


def write_init_file(filepath: Path) -> None:
    """Write __init__.py for data module."""
    content = '''"""
Pre-computed data for pygarble detection strategies.

This module contains embedded lookup tables generated from
Peter Norvig's word frequency data (https://norvig.com/ngrams/).

All data is MIT licensed and can be freely used.
"""

from .words import ENGLISH_WORDS
from .bigrams import BIGRAM_LOG_PROBS, DEFAULT_LOG_PROB
from .trigrams import COMMON_TRIGRAMS

__all__ = [
    "ENGLISH_WORDS",
    "BIGRAM_LOG_PROBS",
    "DEFAULT_LOG_PROB",
    "COMMON_TRIGRAMS",
]
'''
    filepath.write_text(content)
    print(f"Written {filepath}")


def main():
    """Generate all data files."""
    print("=" * 60)
    print("Generating pygarble training data")
    print("=" * 60)

    # Ensure output directory exists
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    # Download source data
    word_freq = download_word_frequencies(NORVIG_WORD_FREQ_URL)

    # Generate derived data
    english_words = generate_english_words(word_freq, max_words=50000)
    bigram_probs = compute_bigram_probabilities(word_freq)
    common_trigrams = compute_trigram_frequencies(word_freq, top_n=2000)

    # Write output files
    write_words_file(english_words, DATA_DIR / "words.py")
    write_bigrams_file(bigram_probs, DATA_DIR / "bigrams.py")
    write_trigrams_file(common_trigrams, DATA_DIR / "trigrams.py")
    write_init_file(DATA_DIR / "__init__.py")

    # Summary
    print("\n" + "=" * 60)
    print("Summary:")
    total_size = sum(f.stat().st_size for f in DATA_DIR.glob("*.py"))
    print(f"  Total data size: {total_size / 1024:.1f} KB")
    print(f"  Words: {len(english_words)}")
    print(f"  Bigrams: {len(bigram_probs)}")
    print(f"  Trigrams: {len(common_trigrams)}")
    print("=" * 60)
    print("Done!")


if __name__ == "__main__":
    main()
